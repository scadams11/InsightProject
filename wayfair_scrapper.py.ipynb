{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import urllib\n",
    "import threading\n",
    "from pymongo import MongoClient\n",
    "from bs4 import BeautifulSoup\n",
    "#from urllib2 import urlopen\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "def get_wayfair_product_links(base_link, num_pages=1):\n",
    "    '''\n",
    "    INPUT: \n",
    "        base_link: string\n",
    "            product index page URL link \n",
    "        num_pages: integer\n",
    "            number of pages that product links will be scraped from \n",
    "            (each page have 48 products)\n",
    "    OUTPUT:\n",
    "        product_links: list of strings\n",
    "            URL links to product pages with product details\n",
    "    '''\n",
    "\n",
    "    product_links = []\n",
    "\n",
    "    for num in range(1,num_pages+1):\n",
    "        html  = urllib.request.urlopen(base_link+str(num))\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        productbox = soup.findAll('a', {'class':'productbox'})\n",
    "        product_links.extend([s['href'] for s in productbox])\n",
    "        \n",
    "    sleep(random.random())\n",
    "    return product_links\n",
    "\n",
    "\n",
    "def wayfair_product_info_scraper(link, category):\n",
    "    '''\n",
    "    INPUT: \n",
    "        link: string\n",
    "            one product URL link \n",
    "        category: string\n",
    "    OUTPUT:\n",
    "        product_info_dict: dictionary\n",
    "            A product info dictionary with the following keys:\n",
    "                * product_id\n",
    "                * website\n",
    "                * category\n",
    "                * url\n",
    "                * title\n",
    "                * price\n",
    "                * colors\n",
    "                * description\n",
    "                * features\n",
    "                * specs         \n",
    "                * manufacturer\n",
    "                * rating_avg\n",
    "                * rating_count\n",
    "                * image_links_all\n",
    "                * image_links_by_color\n",
    "    '''\n",
    "\n",
    "    html  = urllib.request.urlopen(link, timeout=100)\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    product_id = soup.find('span', {'class': 'product_breadcrumb'}).text.split(':')[1]\n",
    "    website = 'wayfair'\n",
    "    url = link\n",
    "    title = soup.find('span', {'class':'title_name'}).text.strip()\n",
    "    price = soup.find('div', {'class':'dynamic_sku_price'}).text.strip()\n",
    "    color_info = soup.findAll('a', {'class': 'js-visual-option'})\n",
    "    colors = [x['data-name'] for x in color_info]\n",
    "    \n",
    "    features_info = soup.findAll('div', {'class':'product_sub_section'})\n",
    "    features = [x.text for x in features_info]\n",
    "       \n",
    "    manufacturer = soup.find('span', {'class':'manu_name'}).text.strip().lstrip('by').strip() \n",
    "        \n",
    "    if soup.find('span', {'class': 'rating_value'}) != None:\n",
    "        rating_avg = soup.find('span', {'class': 'rating_value'}).text\n",
    "    else:\n",
    "        rating_avg = None\n",
    "    \n",
    "    if soup.find('span', {'itemprop':\"reviewCount\"}) != None:\n",
    "        rating_count = soup.find('span', {'itemprop':\"reviewCount\"}).text\n",
    "    else:\n",
    "        rating_count = None\n",
    "    \n",
    "    if soup.find('div', {'class':'spec_dimensions'}) != None:\n",
    "        specs = soup.find('div', {'class':'spec_dimensions'}).text\n",
    "    else:\n",
    "        specs = None\n",
    "     \n",
    "    image_links_all = []\n",
    "    image_links_by_color = {}\n",
    "    \n",
    "    image_info = soup.findAll('a', {'class': 'photoswipe_link'})\n",
    "    image_links = [x['data-original-src'] for x in image_info]\n",
    "    \n",
    "    if len(colors) == 0:\n",
    "        image_links_all = image_links\n",
    "    else:\n",
    "        color_links = [x['href'] for x in color_info]\n",
    "        for i, color_link in enumerate(color_links):\n",
    "            color_key = colors[i]\n",
    "            link_color = link + color_link\n",
    "            html_color  = urllib.request.urlopen(link_color, timeout=100)\n",
    "            soup_color = BeautifulSoup(html_color, 'html.parser')\n",
    "            image_info_color = soup_color.findAll('a', {'class': 'photoswipe_link'})\n",
    "            image_links_color = [x['data-original-src'] for x in image_info_color]\n",
    "            image_links_by_color[color_key] = image_links_color\n",
    "            image_links_all.extend(image_links_color)\n",
    "            sleep(random.random())\n",
    "    image_links_all = list(set(image_links_all))\n",
    "\n",
    "    if soup.find('p', {'class':'product_section_description'}) != None:\n",
    "        description = soup.find('p', {'class':'product_section_description'}).text\n",
    "    else:\n",
    "        description = None\n",
    "\n",
    "    # Add code to scrape missing features data (when the webpage has a different structure):\n",
    "    if len(features) == 0:\n",
    "        description_info = soup.find('div', {'class':'no_json_description'})\n",
    "        if description_info.text != None:\n",
    "            description = description_info.text\n",
    "        else:\n",
    "            description = '\\n'.join([x.text for x in description_info.findAll('p')])\n",
    "        if description_info.ul != None:\n",
    "            features = description_info.ul.text\n",
    "        else:\n",
    "            features = '\\n'.join([x.text for x in soup.findAll('ul') if 'Free Shipping' not in x.text])\n",
    "    # Added code ends\n",
    "\n",
    "    product_info_dict = {}\n",
    "    product_info_dict['product_id'] = product_id\n",
    "    product_info_dict['website'] = website\n",
    "    product_info_dict['category'] = category\n",
    "    product_info_dict['url'] = url\n",
    "    product_info_dict['title'] = title\n",
    "    product_info_dict['price'] = price\n",
    "    product_info_dict['colors'] = colors\n",
    "    product_info_dict['description'] = description\n",
    "    product_info_dict['features'] = features\n",
    "    product_info_dict['specs'] = specs\n",
    "    product_info_dict['manufacturer'] = manufacturer\n",
    "    product_info_dict['rating_avg'] = rating_avg\n",
    "    product_info_dict['rating_count'] = rating_count\n",
    "    product_info_dict['image_links_all'] = image_links_all\n",
    "    product_info_dict['image_links_by_color'] = image_links_by_color\n",
    "    \n",
    "    sleep(random.random())\n",
    "    return product_info_dict\n",
    "\n",
    "\n",
    "def wayfair_image_scraper(indices, category):\n",
    "    '''\n",
    "    Save images from the image URLs.\n",
    "    INPUT: \n",
    "        indices: list of integers\n",
    "        category: string\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    image = urllib.URLopener()\n",
    "    for i in indices:\n",
    "        i = int(i)\n",
    "        product_id = str(df.ix[i,'product_id'].strip())\n",
    "        img_links = df.ix[i,'image_links_all']\n",
    "\n",
    "        for j, link in enumerate(img_links):\n",
    "            image.retrieve(link, 'wayfair/images/%s/%s_%s_%s.jpg' % (category, category, product_id, str(j)))\n",
    "\n",
    "\n",
    "def multithreading_image_scraper(df, category):\n",
    "    '''\n",
    "    INPUT: \n",
    "        df: pandas dataframe \n",
    "        category: string\n",
    "    OUTPUT:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    index = df.index.values\n",
    "    threads = []\n",
    "    for i in xrange(0, 11):\n",
    "        start = i*100\n",
    "        if (i+1)*100 > 1008:       \n",
    "            end = 1008       \n",
    "        else:\n",
    "            end = (i+1)*100\n",
    "        indices = tuple(index[start:end])\n",
    "        t = threading.Thread(target=wayfair_image_scraper, args=(indices, category))\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads: thread.start()\n",
    "    for thread in threads: thread.join()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Base link for each category:\n",
    "    base_link_dict = {}\n",
    "    base_link_dict['sofa'] = 'http://www.wayfair.com/Sofas-C413892.html?&curpage='\n",
    "    base_link_dict['sofa_bed'] = 'http://www.wayfair.com/Sofa-Beds-C413895.html?&curpage='\n",
    "    base_link_dict['futon'] = 'http://www.wayfair.com/Futons-C1780368.html?&curpage='\n",
    "    base_link_dict['loveseat'] = 'http://www.wayfair.com/Loveseats-C413896.html?&curpage='\n",
    "    base_link_dict['coffee_table'] = 'http://www.wayfair.com/Coffee-Tables-C414602.html?&curpage='\n",
    "    base_link_dict['desk'] = 'http://www.wayfair.com/All-Desks-C1780384.html?&curpage='\n",
    "    base_link_dict['office_chair'] = 'http://www.wayfair.com/All-Office-Chairs-C478390.html?&curpage='\n",
    "    base_link_dict['bookcase'] = 'http://www.wayfair.com/All-Bookcases-C1780385.html?&curpage='\n",
    "    base_link_dict['dining_table'] = 'http://www.wayfair.com/Kitchen-and-Dining-Tables-C46129.html?&curpage='\n",
    "    base_link_dict['dining_chair'] = 'http://www.wayfair.com/Kitchen-and-Dining-Chairs-C46130.html?&curpage='\n",
    "    base_link_dict['bed'] = 'http://www.wayfair.com/Beds-C46122.html?&curpage='\n",
    "    base_link_dict['nightstand'] = 'http://www.wayfair.com/Nightstands-C46062.html?&curpage='\n",
    "    base_link_dict['dresser'] = 'http://www.wayfair.com/Dressers-C46091.html?&curpage='\n",
    "\n",
    "    categories = ['sofa', 'sofa_bed', 'futon', 'loveseat', 'coffee_table', 'desk', 'office_chair', \n",
    "                    'dining_table', 'dining_chair', 'bookcase', 'nightstand', 'bed', 'dresser']\n",
    "\n",
    "    # Product info scraping and saving into MongoDB:\n",
    "    for category in categories:\n",
    "        client = MongoClient()\n",
    "        db = client['furniture']\n",
    "        collection = db[category]\n",
    "\n",
    "        product_links = get_wayfair_product_links(base_link_dict[category], num_pages=20)\n",
    "        for link in product_links:\n",
    "            product_dict = wayfair_product_info_scraper(link, category_input=category)\n",
    "            collection.insert_one(product_dict)\n",
    "\n",
    "    # Image scraping: \n",
    "    for category in categories:\n",
    "        client = MongoClient()\n",
    "        db = client['furniture']\n",
    "        collection = db[category]\n",
    "        df = pd.DataFrame(list(collection.find()))\n",
    "        multithreading_image_scraper(df, category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'product_links' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-77402213bd73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mproduct_links\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'product_links' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
